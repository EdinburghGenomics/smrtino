## BLOB plotter rules ##
# vim: ft=python

from smrtino import load_yaml, dump_yaml

# I'll copy the logic from Snakefile.common_denovo here

# Default is that there will be 10000 sequences subsampled
# and they will be blasted in 100 chunks.
# To override set, eg., EXTRA_SNAKE_FLAGS="--config blob_subsample=1234"

# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = int(config.get('blob_subsample', 10000))
BLOB_CHUNKS    = int(config.get('blob_chunks', 100))
BLOB_LEVELS    = config.get('blob_levels', "phylum order species".split())
BLAST_SCRIPT   = config.get('blast_script', "blast_nt")

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 blob plots and thumbnails for
# each, and arranges the plots into two rows of three columns as we wish to
# display them.
#
# Within the blob directory I'm not having {cell}/{barcode} subdirs - just
# putting everything in one place.
#
localrules: list_blob_plots, get_blob_species
rule list_blob_plots:
    output: "blob/{cell}.{part}.{barcode}.plots.yaml"
    input:
        png = lambda wc: expand( "blob/{cell}.{part}.{barcode}.{taxlevel}.{extn}{thumb}.png",
                                 cell = [wc.cell],
                                 part = [wc.part],    # hifi_reads
                                 barcode = [wc.barcode],
                                 taxlevel = BLOB_LEVELS,
                                 extn = "cov0 read_cov.cov0".split(),
                                 thumb = ['.__thumb', ''] ),
        subs = "blob/{cell}.{part}.{barcode}+sub" + str(BLOB_SUBSAMPLE) + ".fasta",
    run:
        # Un-silence sys.stderr in sub-jobs:
        logger.quiet.discard('all')

        wc = wildcards
        # We want to know how big the subsample actually was, so check the FASTQ
        count_from_fastq = int(next(shell("grep -o '^>' {input.subs} | wc -l", iterable=True)))

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here but I can make a nested list comprehension.
        plot_files = [ expand( "{cell}.{part}.{barcode}.{taxlevel}.{extn}.png",
                               cell     = [wc.cell],
                               part     = [wc.part],
                               barcode  = [wc.barcode],
                               taxlevel = BLOB_LEVELS,
                               extn     = [extn] )
                       for extn in "read_cov.cov0 cov0".split() ]

        plots = dict(title = f"Taxonomy for {wc.part} {wc.barcode} ({count_from_fastq} sequences)"
                             f" by {', '.join(BLOB_LEVELS)}",
                     files = plot_files )

        dump_yaml([plots], filename=str(output))

rule get_blob_species:
    output: "blob/{cell}.{part}.{barcode}.species.txt"
    input:
        txt = expand("blob/{{cell}}.{{part}}.{{barcode}}.{bl}.blobplot.stats.txt", bl=BLOB_LEVELS[::-1])
    shell:
        "blobplot_stats_to_species.py {input.txt} > {output}"

# Convert to FASTA and subsample and munge the headers
rule bam_to_subsampled_fasta:
    output: "blob/{cell}.{part}.{barcode}+sub{n}.fasta"
    input:  "{cell}/{barcode}/{cell}.{part}.{barcode}.bam"
    threads: 8
    resources:
        mem_mb = 44000,
        n_cpus = 8,
    shell:
        "{TOOLBOX} samtools fasta -@ {threads} {input} | {TOOLBOX} seqtk sample - {wildcards.n} | sed 's,/,_,g' > {output}"


# Makes a .complexity file for our FASTA file
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 (or however many) blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB - there can
# be a _lot_ of repeats so this may be worth running on the cluster.
localrules: merge_blast_reports
def i_merge_blast_reports(wildcards):
    """Return a list of BLAST reports to be merged based upon how many chunks
       were outputted by split_fasta_in_chunks.
    """
    chunks_list_file = checkpoints.split_fasta_in_chunks.get(**wildcards).output.list
    with open(chunks_list_file) as fh:
        fasta_chunks = [ l.rstrip('\n') for l in fh ]
    # Munge the list of FASTA chunks to get the list of required BLAST chunks
    return dict( bparts =
                    [ re.sub(r'\.fasta_parts/', '.blast_parts/',
                             re.sub(r'\.fasta$', '.bpart', c))
                      for c in fasta_chunks ] )

rule merge_blast_reports:
    output: "blob/{foo}.blast"
    input:  unpack(i_merge_blast_reports)
    shell:
        'LC_ALL=C ; ( for i in {input.bparts} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("blob/{foo}.blast_parts/{chunk}.bpart")
    input:  "blob/{foo}.fasta_parts/{chunk}.fasta"
    threads: 4
    resources:
        mem_mb = 24000,
        n_cpus = 6,
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} {BLAST_SCRIPT} -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA into (at most) BLOB_CHUNKS chunks. The number may be less than BLOB_CHUNKS so this
# is a checkpoint rule, and merge_blast_reports then responds to the variable number of outputs. Note
# this will even 'split' a completely empty file if you ask it to, and make zero output files plus
# an empty output.parts.
checkpoint split_fasta_in_chunks:
    output:
        list = "blob/{foo}.fasta_parts_list",
        parts = temp(directory("blob/{foo}.fasta_parts")),
    input: "blob/{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """mkdir {output.parts}
           touch {output.parts}/list
           awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{output.parts}/part_%04d.fasta", n_file); n_file++; \
                         print file >> "{output.parts}/list"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           mv {output.parts}/list {output.list}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    resources:
        mem_mb = 30000,
        n_cpus = 6,
    shell:
       r"""if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/{wildcards.foo} \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/{wildcards.foo}.blobDB.json {output.json}
        """

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of PNG files. Note I gave up on hacking BLOBTools to make sensible
# sized images I just downsample them with GM.
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png", "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        maxsize = "1750x1750",
        thumbsize = "320x320"
    shadow: 'shallow'
    resources:
        mem_mb = 30000,
        n_cpus = 6,
    shell:
       r""" mkdir blob_tmp
            if [ -s {input.json} ] ; then
                export BLOB_COVERAGE_LABEL=Non-Dustiness
                {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --sort_first no-hit,other,undef -r {wildcards.taxlevel}
                ls -l blob_tmp
                mv blob_tmp/{wildcards.foo}.*.stats.txt {output.stats}

                {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.cov0.png \
                    -resize {params.maxsize}'>' {output.plotc[0]}

                {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png \
                    -resize {params.maxsize}'>' {output.plotr[0]}
            else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
            fi
            {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize}'>' {output.plotc[1]}
            {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize}'>' {output.plotr[1]}
        """

# The blob/*.fasta_parts directories are not getting removed. I think this is because they are outputs of
# a checkpoint rule. Remove them here.
onsuccess:
    logger.quiet.discard('all')
    if config.get('cleanup'):
        shell("rm -rvf ./blob/*.fasta_parts")

## End of BLOB plotter rules ##

