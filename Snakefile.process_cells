#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import xml.etree.ElementTree as ET
from snakemake.io import Wildcards
from glob import glob
from smrtino import load_yaml, dump_yaml


""" This is a recipe to process raw SMRT cells from the PacBio Revio,
    and start to calculate some stats.

    The script wants to be run in an output dir under $TO_LOCATION
    It will process all SMRT cells specified by config['cells']
    If should (maybe?) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])
PBCORETOOLS = TOOLBOX + " smrt python3 -m pbcoretools.tasks"

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs).
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# Should not be necessary to give the run dir as pbpipeline/from should point to it...
RUNDIR = config.get('rundir', 'pbpipeline/from')

# Load the info prepared by scan_cells.py
try:
    SC = load_yaml(config.get("sc_data", "sc_data.yaml"))
except Exception:
    # Allow for cases where we just want to run one rule and don't care about the sc_data
    SC = dict(cells={})

logger.info(f"SC =\n{SC!r}")

def find_source_file(**args):
    """ Shim to get me a source file by looking into SC.

        This returns a function which can be used as an input function.
    """
    def _fsf(wildcards):
        # combine args and wildcards. This is an undocumented use of the snakemake.io.Wildcards
        # constructor to combine args with wildcards.
        wc = Wildcards(wildcards, args)
        try:
            if wc.barcode == "unassigned":
                # Special case when "unassigned" is used as a barcode name.
                fn = SC['cells'][wc.cell]['unassigned'][wc.part][wc.extn]
            else:
                fn = SC['cells'][wc.cell]['barcodes'][wc.barcode][wc.part][wc.extn]

        except AttributeError:
            # No barcode. Must be the metadata file.
            fn = SC['cells'][wc.cell][wc.part]

        return f"{RUNDIR}/{fn}"

    return _fsf

def copy_all_files(i, o, keys=None):
    """ For rules that like to copy stuff
        i and o are dicts of filenames with common keys, which
        works for rule input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

def link_all_files(i, o, keys=None):
    """ Copying is not necessary if everything is on the same FS,
        so have a symlink option. But probably we should have one
        copy on /lustre-egdp and another on /lustre-gseg.
        i and o are dicts of filenames with common keys, which
        works for rule input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        shell("ln -sn {infile} {outfile}")

wildcard_constraints:
    cell = r"m\w+",
    part = r"hifi_reads|fail_reads",

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info
rule main:
    input:
        yaml     = [ f"{c}.info.yaml" for c in SC['cells'] ]

def i_one_cell_info(wc):
    """The summary for a cell is just the summaries for all barcodes, plus unassigned,
       combined with the run info from sc_data.yaml
       The info we need to make this list is in SC.
    """
    cell = wc.cell
    cell_barcodes = sorted(SC['cells'][cell]['barcodes'])

    res = dict( sc_data = "sc_data.yaml",
                unass   = f"{cell}/unassigned/{cell}.info.unassigned.yaml")

    res['bc'] = [ f"{cell}/{bc}/{cell}.info.{bc}.yaml"
                  for bc in cell_barcodes ]

    return res

# This rule connects the .info.yaml to all the bits of data we need and also generates
# the .info.yaml contents for a barcode.
def i_one_barcode_info(wc):
    """See what we need to generate for a single barcode. This used to be for a cell but now
       accounts for barcodes by processing each barcode singly.
    """
    '''
    scinfo = SC['cells'][cell]
    barcodes = scinfo['barcodes']
    barcodes_and_unass = dict( unassigned = scinfo['unassigned'],
                               **barcodes )
    '''
    cell = wc.cell
    barcode = wc.bc
    parts = SC['cells'][cell]['parts']

    res = dict( md5 = [],
                cstats = [],
                cou = [],
                xml = [],
                blobs = [],
                taxon = [] )

    # We need md5sums and contig stats for hifi_reads and fail_reads
    for part in parts:
        res['md5'].append(f"md5sums/{cell}/{barcode}/{cell}.{part}.{barcode}.bam.md5")
        res['cstats'].append(f"{cell}/{barcode}/{cell}.{part}.{barcode}.cstats.yaml")

        # Also the consensusreadset.xml
        res['xml'].append(f"{cell}/{barcode}/{cell}.{part}.{barcode}.consensusreadset.xml")

    for apart, rsname in dict( subreads = ".subreadset.xml",
                               reads    = ".consensusreadset.xml" ).items():
        if apart in scinfo['parts']:
            # this was input.xml2
            res['xml'].append(cellbase + rsname)

    # Also the .m64175e_221012_112607.run.metadata.xml file which we copy without the '.'
    res['runxml'] = f"{wc.cell}.run.metadata.xml"

    # If we have CCS reads then we also want to extract the HiFi reads and count
    # the number of sequences. Also be running cstats on this.
    if 'reads' in scinfo['parts']:
        res['cou'].append(cellbase + ".hifi_reads.fastq.count")
        res['md5'].append("md5sums/" + cellbase + ".hifi_reads.fastq.gz.md5")
        # Javier confirms we want FASTA+FASTQ+BAM
        res['md5'].append("md5sums/" + cellbase + ".hifi_reads.fasta.gz.md5")
        res['md5'].append("md5sums/" + cellbase + ".hifi_reads.bam.md5")
        # And we want stats for the BAM, as well as counts for the FASTQ
        res['cstats'].append(cellbase + ".hifi_reads.cstats.yaml")

    # Blobs or no?
    if str(config.get('blobs', '1')) != '0':
        res['blobs'].append("blob/" + cellbase + ".plots.yaml")
        res['taxon'].append("blob/" + cellbase + ".species.txt")

    return res

rule one_cell_info:
    output: "{cell}.info.yaml"
    input:  unpack(i_one_cell_info)
    shell:
       r"""combine_cell_info.py \
                --sc_data {input.sc_data} \
                --unass {input.unass} \
                {input.bc} > {output}
        """

rule one_barcode_info:
    output: "{cell}/{bc}/{cell}.info.{bc}.yaml"
    input:  unpack(i_one_barcode_info)
    shell:
       # What needs to go into the YML? Stuff from the XML and also some stuff from the
       # stats, maybe? At present the script will discover extra files automagically.
       r"""compile_barcode_info.py {input.xml[0]} \
             --runxml {input.runxml} \
             --stats  {input.cstats} \
             --taxon  {input.taxon} \
             --plots  {input.blobs} > {output}
        """

# On GSEG this had to be on the login node as the worker nodes can't see IFS.
# Still a useful option to have.
if os.environ.get('FILTER_LOCALLY', '1') != '0':
    localrules: copy_reads

def relativize_xml(xfile):
    """Munge an XML file to reflect new file paths and remove unused references
       so that the file can be imported into another SMRTLink.
    """
    shell("mv -v {xfile} {xfile}.orig")
    shell("strip_readset_resources.py {xfile}.orig > {xfile}")
    shell("{TOOLBOX} smrt dataset --skipCounts --log-level INFO relativize {xfile}")
    shell("rm -v {xfile}.orig")

# This copies/links the files for a given barcode, then fixes up the XML.
# part may be "hifi_reads" (ie. pass) or "fail_reads".
rule copy_reads:
    output:
        bam    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam",
        pbi    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam.pbi",
        xml    = "{cell}/{barcode}/{cell}.{part}.{barcode}.consensusreadset.xml",
    input:
        bam    = find_source_file(fmt="bam"),
        pbi    = find_source_file(fmt="pbi"),
        xml    = find_source_file(fmt="xml"),
    resources:
        nfscopy=1
    run:
        link_all_files(input, output, keys="bam pbi".split())
        copy_all_files(input, output, keys="xml".split())
        relativize_xml(output.xml)

rule copy_meta:
    output:
        meta   = "{cell}.metadata.xml"
    input:
        meta   = find_source_file(fmt="meta")
    run:
        copy_all_files(input, output)

# This script produces some headline stats as well as a histogram we can use
# (but currently we don't)
rule get_contig_stats_yaml:
    output:
        yaml = "{bam}.cstats.yaml",
        histo = "histo/{bam}.length_histo.tsv"
    input:  "{bam}.bam"
    shell:
        "fasta_stats.py -H {output.histo} <({TOOLBOX} samtools fasta {input}) > {output.yaml}"


# Export the HiFi reads as FASTQ
rule hifi_fastx:
    output:
        fastq = "{cell}.hifi_reads.fastq.gz",
    input:
        xml = "{cell}.hifireadset.xml",
        bam = "{cell}.reads.bam",
    threads: 4
    resources:
        mem_mb = 24000,
        n_cpus = 4,
    shell:
        "{TOOLBOX} smrt bam2{wildcards.x} -j {threads} -o {wildcards.cell}.hifi_reads {input.xml}"

# Make a .count file for the FASTQ file
rule count_fastq:
    output: "{foo}.fastq.count"
    input:  "{foo}.fastq.gz"
    shell:  "fq_base_counter.py {input} > {output}"

# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: '( cd "$(dirname {input:q})" && md5sum -- "$(basename {input:q})" ) > {output:q}'

## BLOB plotter rules ##

# I'll copy the logic from Snakefile.common_denovo here

# Default is that there will be 10000 sequences subsampled
# and they will be blasted in 100 chunks.
# To override set, eg., EXTRA_SNAKE_FLAGS="--config blob_subsample=1234"

# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = int(config.get('blob_subsample', 10000))
BLOB_CHUNKS    = int(config.get('blob_chunks', 100))
BLOB_LEVELS    = config.get('blob_levels', "phylum order species".split())
BLAST_SCRIPT   = config.get('blast_script', "blast_nt")

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 blob plots and thumbnails for
# each, and arranges the plots into two rows of three columns as we wish to
# display them.
localrules: list_blob_plots, get_blob_species
rule list_blob_plots:
    output: "blob/{cell}{filter}.plots.yaml"
    input:
        png = lambda wc: expand( "blob/{cell}{flt}.{s}.{taxlevel}.{extn}{thumb}.png",
                                 cell = [wc.cell],
                                 flt = [wc.filter],
                                 s = SC[wc.cell]['parts'], # subreads/scraps/reads
                                 taxlevel = BLOB_LEVELS,
                                 extn = "cov0 read_cov.cov0".split(),
                                 thumb = ['.__thumb', ''] ),
        subs = lambda wc: expand( "blob/{cell}{flt}.{s}+sub{size}.fasta",
                                  cell = [wc.cell],
                                  flt = [wc.filter],
                                  s = SC[wc.cell]['parts'],
                                  size = [BLOB_SUBSAMPLE] )
    run:
        # We want to know how big the subsample actually was, so check the FASTQ
        counts = dict()
        for apart, asubfile in zip(SC[wildcards.cell]['parts'], input.subs):
            counts[apart] = int(next(shell("grep -o '^>' {asubfile} | wc -l", iterable=True)))

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here but I can make a nested list comprehension.
        plots = [ dict(title = 'Taxonomy for {} ({} sequences) by {}'.format(
                                        s,
                                        counts[s],
                                        ', '.join(BLOB_LEVELS) ),
                       files = [ [ "{cell}{f}.{s}.{taxlevel}.{extn}.png".format( cell = wildcards.cell,
                                                                                 f = wildcards.filter,
                                                                                 s = s,
                                                                                 taxlevel = taxlevel,
                                                                                 extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]

                      ) for s in SC[wildcards.cell]['parts'] ]
        with open(output[0], 'w') as ofh:
            print(yaml.safe_dump(plots), file=ofh)

rule get_blob_species:
    output: "blob/{cell}{filter}.species.txt"
    input:
        txt = lambda wc: expand( "blob/{cell}{flt}.{s}.{taxlevel}.blobplot.stats.txt",
                                 cell = [wc.cell],
                                 flt = [wc.filter],
                                 s = SC[wc.cell]['parts'], # subreads/scraps/reads
                                 taxlevel = BLOB_LEVELS[::-1] )
    shell:
        "blobplot_stats_to_species.py {input.txt} > {output}"

# Convert to FASTA and subsample and munge the headers
rule bam_to_subsampled_fasta:
    output: "blob/{foo}+sub{n}.fasta"
    input: "{foo}.bam"
    shell:
        "{TOOLBOX} samtools fasta {input} | {TOOLBOX} seqtk sample - {wildcards.n} | sed 's,/,_,g' > {output}"


# Makes a .complexity file for our FASTA file
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
localrules: merge_blast_reports
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        "cat {input} > {output}"

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    resources:
        mem_mb = 24000,
        n_cpus = 6,
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} {BLAST_SCRIPT} -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.list} {output.parts}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    resources:
        mem_mb = 30000,
        n_cpus = 6,
    shell:
       r"""if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/{wildcards.foo} \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/{wildcards.foo}.blobDB.json {output.json}
        """

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of PNG files. Note I gave up on hacking BLOBTools to make sensible
# sized images I just downsample them with GM.
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png", "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        maxsize = "1750x1750",
        thumbsize = "320x320"
    shadow: 'shallow'
    resources:
        mem_mb = 30000,
        n_cpus = 6,
    shell:
       r""" mkdir blob_tmp
            if [ -s {input.json} ] ; then
                export BLOB_COVERAGE_LABEL=Non-Dustiness
                {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --sort_first no-hit,other,undef -r {wildcards.taxlevel}
                ls -l blob_tmp
                mv blob_tmp/{wildcards.foo}.*.stats.txt {output.stats}

                {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.cov0.png \
                    -resize {params.maxsize}'>' {output.plotc[0]}

                {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png \
                    -resize {params.maxsize}'>' {output.plotr[0]}
            else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
            fi
            {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize}'>' {output.plotc[1]}
            {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize}'>' {output.plotr[1]}
        """


## End of BLOB plotter rules ##

