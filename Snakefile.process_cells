#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import yaml
import xml.etree.ElementTree as ET
from snakemake.utils import format
from glob import glob

""" So, this is a recipe to process raw SMRT cells from the PacBio Sequel,
    and start to calculate some stats.

    The script wants to be run in an output dir on /lustre.
    It will process all SMRT cells specified by config['cells']
    If should (!TESTME!) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])
PBCORETOOLS = TOOLBOX + " smrt python3 -m pbcoretools.tasks"

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs):
#   fasta_stats.py
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundir', 'pbpipeline/from')

def scan_cells():
    """ Work out all the cells to process based on config['cells'] and config['rundir']
        and thus infer the base names  of the info.yml files that need to be made.
        Return a dict of:
            cell_id->{'slot': slot_id, 'filter': '', 'parts': [...]}.
        This in turn allows me to work out everything else by pattern matching. Note that
        no filter is currently supported but I left this feature in just in case.
        When run on a GSEG worker node this list will come out empty, but that's
        OK.
    """
    all_done = [ b[:-len('.transferdone')] for b in glob('{}/*/*.transferdone'.format(RUNDIR) ) ]

    # Now see if I need to filter by config['cells']
    if 'cells' in config:
        res = [ s for s in all_done if s.split('/')[-2] in config['cells'].split() ]
    else:
        res = all_done

    if not ('--no-hooks' in sys.argv or config.get('ignore_missing')):
        # A hacky way of having an assertion only on the master process.
        assert res, "No subreads found in {} matching cell [{}].".format(RUNDIR, config.get('cells', '*'))

    return { r.split('/')[-1]: { 'slot':   r.split('/')[-2],
                                 'filter': determine_filter(r),
                                 'parts':  determine_parts(r) } for r in res }

def determine_filter(cellpath):
    """ Work out what filter needs to be applied to this cell. The cell is supplied
        as a full path, since scan_cells() already worked that out.
    """
    # See git commit 1f5ab22383d355e112d31430cc0d8026fc6f5c17 for the last time
    # this did anything
    return ''

def determine_parts(cellpath):
    """Work out if this is a ['subreads', 'scraps'] cell or a ['reads'] cell.
    """
    bamfiles = glob(cellpath + '.*.bam')
    parts = [ b[len(cellpath + '.'):-len('.bam')] for b in bamfiles ]

    assert parts, 'No .bam files found matching ' + cellpath + '.*.bam'
    # Fix the order. This works.
    return sorted(parts, reverse=True)

# And run the scan just once
SC = scan_cells()
try:
    logger.info("SC = {!r}".format(SC))
except NameError:
    pass

def find_source_file(extn):
    """ Shim to get me the source file by adding in the cell slot directory as discovered
        by scan_cells()
        I had a different workaround involving linking files but this is more direct.
        This returns a function which can be used as an input function.
    """
    def _ff(wildcards):
        return "{}/{}/{}{}".format( RUNDIR,
                                    SC[wildcards.cell]['slot'],
                                    wildcards.cell,
                                    extn.format(**vars(wildcards)) )
    return _ff

def copy_all_files(i, o):
    """ For rules that like to copy stuff
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in o.keys() ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info
rule main:
    input:
        yaml     = [ c + '.info.yml' for c in SC ]

# This rule connects the .info.yml to all the bits of data we need and also generates
# the .info.yml contents.
def i_one_cell_info(wildcards):
    """The inputs for one_cell_info get complex since we need to account for whether
       filters need to be applied, and if so we need both the filtered and unfiltered .xml files.
       Also we now need to worry about the difference between cells that have been CCS-processed
       and the old subreads+scraps.
    """
    # get slot + filter + type from SC
    scinfo = SC[wildcards.cell]
    cellbase = wildcards.cell + scinfo['filter']
    res = dict( md5 = [],
                cstats = [],
                count = [],
                xml = [],
                blobs = [],
                hplots = [] )

    # This will loop over ['subreads', 'scraps'] or ['reads']
    for apart in scinfo['parts']:
        res['md5'].append("md5sums/" + cellbase + "." + apart + ".bam.md5")
        res['cstats'].append(cellbase + "." + apart + ".cstats.yml")

    # We may need 1 or 2 .subreadset.xml depending on the filter
    # For .consensusreadset.xml no filters are actually supported but they could be.
    for apart, rsname in dict( subreads = ".subreadset.xml",
                               reads    = ".consensusreadset.xml" ).items():
        if apart in scinfo['parts']:
            # this was input.xml2
            res['xml'].append(cellbase + rsname)
            if scinfo['filter']:
                # this was input.xml1
                res['xml'].append(wildcards.cell + rsname)

    # If we have CCS reads then we also want to extract the HiFi reads and count
    # the number of sequences. Maybe we should be running cstats on this??
    if 'reads' in scinfo['parts']:
        res['count'].append(cellbase + ".hifi_reads.fastq.count")

    # Length histograms
    res['hplots'].append("histo/" + cellbase + ".plots.yml"),

    # Blobs or no?
    if str(config.get('blobs', '1')) != '0':
        res['blobs'].append("blob/" + cellbase + ".plots.yml")

    return res

rule one_cell_info:
    output: "{cell}.info.yml"
    input:  unpack(i_one_cell_info)
    shell:
        # What needs to go into the YML? Stuff from the XML and also some stuff from the
        # stats, maybe? At present the script will discover extra files automagically.
        r'''compile_cell_info.py {input.xml[0]} \
             -s {input.cstats} \
             -p {input.hplots} {input.blobs} > {output}
        '''

# On GSEG this has to be local as the worker nodes can't see IFS...
if os.environ.get('FILTER_LOCALLY', '1') != '0':
    localrules: filter_control, copy_subreads, copy_reads, copy_xml

def relativize_xml(xfile):
    """Munge an XML file to reflect new file paths.
    """
    shell("mv -v {xfile} {xfile}.orig")
    shell("strip_readset_resources.py {xfile}.orig > {xfile}")
    shell("{TOOLBOX} smrt dataset --skipCounts --log-level INFO relativize {xfile}")
    shell("rm -v {xfile}.orig")

# This copies the files, without applying any filter.
rule copy_subreads:
    output:
        bam    = "{cell,[^./]+}.subreads.bam",
        pbi    = "{cell,[^./]+}.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.scraps.bam",
        xml    = "{cell,[^./]+}.subreadset.xml"
    input:
        bam    = find_source_file(".subreads.bam"),
        pbi    = find_source_file(".subreads.bam.pbi"),
        scraps = find_source_file(".scraps.bam"),
        xml    = find_source_file(".subreadset.xml")
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)
        relativize_xml(output.xml)

# Ditto for reads.bam
rule copy_reads:
    output:
        bam    = "{cell,[^./]+}.reads.bam",
        pbi    = "{cell,[^./]+}.reads.bam.pbi",
        xml    = "{cell,[^./]+}.consensusreadset.xml"
    input:
        bam    = find_source_file(".reads.bam"),
        pbi    = find_source_file(".reads.bam.pbi"),
        xml    = find_source_file(".consensusreadset.xml")
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)
        relativize_xml(output.xml)


# This applies the control removal filter via bam2bam, resulting in files with .nocontrol added to
# the name and new xml/pbi files.
# Note that bam2bam is very much deprecated - nee note in toolbox!
rule filter_control:
    output:
        bam    = "{cell,[^./]+}.nocontrol.subreads.bam",
        pbi    = "{cell,[^./]+}.nocontrol.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.nocontrol.scraps.bam",
        xml    = "{cell,[^./]+}.nocontrol.subreadset.xml"
    input:
        bam    = find_source_file(".subreads.bam"),
        scraps = find_source_file(".scraps.bam"),
    resources:
        nfscopy=1
    shell:
        # Filter with standard sequences (would we ever use anything else??)
        "{TOOLBOX} bam2bam {input.bam} {input.scraps} -o {wildcards.cell}.nocontrol"

# This script produces some headline stats as well as a histogram we'll use in the next rule
rule get_contig_stats_yaml:
    output:
        yaml = "{bam}.cstats.yml",
        histo = "histo/{bam}.length_histo.tsv"
    input:  "{bam}.bam"
    shell:
        "fasta_stats.py -H {output.histo} <({TOOLBOX} samtools fasta {input}) > {output.yaml}"

rule histo_plot:
    output:
        hplot = "{bam}.length_histo.png",
        thumb = "{bam}.length_histo.__thumb.png"
    input:
        histo = "{bam}.length_histo.tsv"
    params:
        thumbsize = "480x480",
        bins = "1K,2K,3K,5K,10K,50K,100K"
    shell:
        '''{TOOLBOX} plot_histo.R -f {input} -o {output.hplot} --bins {params.bins} \
                --title `basename {input} .length_histo.tsv`
           {TOOLBOX} convert {output.hplot} -resize {params.thumbsize} {output.thumb}
        '''

# Note the auto_ccs_outputs step should produce the .hifi_reads.fastq.gz file as the default output
# name.
rule hifi_fastq:
    output:
        fastq = "{cell}.hifi_reads.fastq.gz",
        count = "{cell}.hifi_reads.fastq.count",
        xml   = "{cell}.hifireadset.xml",
        json  = "{cell}.fastq.datastore.json"
    input:
        xml = "{cell}.consensusreadset.xml",
        bam = "{cell}.reads.bam"
    params:
        filter_args = "--min-qv 20"
    shell:
       r"""{PBCORETOOLS}.dataset_filter {params.filter_args} {input.xml} {output.xml} ''
           ( {PBCORETOOLS}.auto_ccs_outputs --log-level INFO {params.filter_args} fastq {output.xml} {output.json} ; sync ) &

           job_pid=$!

           # TODO - allow fq_base_counter.py to read from stdin and use a dummy name so this works...
           tail -f --retry --pid="$gzip_pid" -c+0 {output.fastq} | zcat | fq_base_counter.py -s {output.fastq} > {output.count}
        """

# Make a .count file for the FASTQ file. Maybe I should use fasta_stats.py for this instead?
# Note that I'm now running this as the file is made, for speed!
"""
rule count_fastq:
    output: "{foo}.fastq.count"
    input:  "{foo}.fastq.gz"
    shell:  "fq_base_counter.py {input} > {output}"
"""

## BLOB plotter rules ##

# I'll copy the logic from Snakefile.common_denovo here

# I'm going to hard-code that there will be 10000 sequences subsampled
# and they will be blasted in 100 chunks.

# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 blob plots and thumbnails for
# each, and arranges the plots into two rows of three columns as we wish to
# display them.
localrules: list_blob_plots
rule list_blob_plots:
    output: r"blob/{cell,\w+}{filter,\.[a-z]+|}.plots.yml"
    input:
        png = lambda wc: expand( "blob/{cell}.{s}.{taxlevel}.{extn}{thumb}.png",
                                 cell = [wc.cell + wc.filter],
                                 s = SC[wc.cell]['parts'], # subreads/scraps/reads
                                 taxlevel = BLOB_LEVELS,
                                 extn = "cov0 read_cov.cov0".split(),
                                 thumb = ['.__thumb', ''] ),
        subs = lambda wc: expand( "blob/{cell}.{s}+sub{size}.fasta",
                                  cell = [wc.cell + wc.filter],
                                  s = SC[wc.cell]['parts'],
                                  size = [BLOB_SUBSAMPLE] )
    run:
        # We want to know how big the subsample actually was, so check the FASTQ
        counts = dict(zip( SC[wildcards.cell]['parts'],
                           [ l.split()[0] for l in
                             shell("grep -o '^>' {input.subs} | uniq -c", iterable=True) ] ))

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here but I can make a nested list comprehension.
        plots = [ dict(title = 'Taxonomy for {} ({} sequences) by {}'.format(
                                        s,
                                        counts[s],
                                        ', '.join(BLOB_LEVELS) ),
                       files = [ [ "{cell}{f}.{s}.{taxlevel}.{extn}.png".format( cell = wildcards.cell,
                                                                                 f = wildcards.filter,
                                                                                 s = s,
                                                                                 taxlevel = taxlevel,
                                                                                 extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]

                      ) for s in SC[wildcards.cell]['parts'] ]
        with open(output[0], 'w') as ofh:
            print(yaml.safe_dump(plots), file=ofh)

# As above but for the histo plots
# As there is only one heading this returns a singleton list.
localrules: list_histo_plots
rule list_histo_plots:
    output: r"histo/{cell,\w+}{filter,\.[a-z]+|}.plots.yml"
    input:
        png = lambda wc: expand( "histo/{cell}.{s}.length_histo{thumb}.png",
                                 cell = [wc.cell + wc.filter],
                                 s = SC[wc.cell]['parts'], # subreads/scraps/reads
                                 thumb = ['.__thumb', ''] )
    run:
        plots = [  dict(title = 'Binned sequence lengths',
                        files = [ [ "{cell}{f}.{s}.length_histo.png".format( cell = wildcards.cell,
                                                                             f = wildcards.filter,
                                                                             s = s )
                                    for s in SC[wildcards.cell]['parts'] ] ]
                       ) ]
        with open(output[0], 'w') as ofh:
            print(yaml.safe_dump(plots), file=ofh)

# Convert to FASTA and subsample and munge the headers
rule bam_to_subsampled_fasta:
    output: "blob/{foo}+sub{n}.fasta"
    input: "{foo}.bam"
    shell:
        "{TOOLBOX} samtools fasta {input} | {TOOLBOX} seqtk sample - {wildcards.n} | sed 's,/,_,g' > {output}"


# Makes a .complexity file for our FASTA file
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        "cat {input} > {output}"

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} blast_nt -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
       r"""mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/{wildcards.foo} \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/{wildcards.foo}.blobDB.json {output.json}
        """

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of PNG files. Note I gave up on hacking BLOBTools to make sensible
# sized images I just downsample them with GM.
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png", "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        maxsize = "1750x1750",
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
       r""" mkdir blob_tmp
            export BLOB_COVERAGE_LABEL=Non-Dustiness
            {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --sort_first no-hit,other,undef -r {wildcards.taxlevel}
            ls -l blob_tmp
            mv blob_tmp/{wildcards.foo}.*.stats.txt {output.stats}

            {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.cov0.png \
                -resize {params.maxsize}'>' {output.plotc[0]}
            {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.cov0.png \
                -resize {params.thumbsize}'>' {output.plotc[1]}

            {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png \
                -resize {params.maxsize}'>' {output.plotr[0]}
            {TOOLBOX} convert blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png \
                -resize {params.thumbsize}'>' {output.plotr[1]}
        """


## End of BLOB plotter rules ##


# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: "( cd `dirname {input}` && md5sum `basename {input}` ) > {output}"
