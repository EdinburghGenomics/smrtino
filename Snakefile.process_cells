#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

# This may change in future. Do we need to do the initial filter/copy op
# as a local job?
export FILTER_LOCALLY="$(is_new_cluster && echo 1 || echo 0)"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import yaml
from snakemake.utils import format
from glob import glob

""" So, this is a recipe to process raw SMRT cells from the PacBio Sequel.

    At first, we're not asking for any QC metrics but we do want to:

    1) Zip and md5sum the subreads.bam
    2) Filter out control sequences (if not already done)

    The script wants to be run in an output dir on /lustre.
    It will process all SMRT cells specified by config['cells']
    If should (!) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])
BAM2BAM = TOOLBOX + 'bam2bam'

RUNDIR  = config['rundir']

def scan_cells(extn=''):
    """ Work out all the cells to process based on config['cells'] and config['rundir']
        and thus the base names  of the info.yml files that need to be made.
        This in turn allows me to work out everything else by pattern matching.
        When run on a GSEG worker node this list will come out empty, but that's
        OK.
    """
    all_subreads = [ b[:-len('.subreads.bam')] for b in glob('{}/*/*.subreads.bam'.format(RUNDIR) ) ]

    # Now see if I need to filter by config['cells']
    if cells in config:
        res = [ s for s in all_subreads if b.split('/')[-2] in config['cells'].split() ]
    else:
        res = all_subreads

    if not '--no-hooks' in sys.argv:
        # A hacky way of having an assertion only on the master process.
        assert res, "No subreads found in {} matching cell [{}].".format(RUNDIR, config.get('cells', '*'))

    return [ r + extn for r in res ]

# Possibly I should start with a file named '.unfiltered' and then the final
# file will just be foo.subreads.bam? We'll see.
FILTER = '.nocontrol'

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info, link_file
rule main:
    input:
        yaml = scan_cells(extn='.info.yml')

# This rule connects the .info.yml to all the bits of data we need and also generates
# the .info.yml contents.
rule one_cell_info:
    output: "{cell}.info.yml"
    input:
        bam_md5    = format("md5sums/{{cell}}{FILTER}.subreads.bam"),
        scraps_md5 = format("md5sums/{{cell}}{FILTER}.scraps.bam"),
        xml        = format("{{cell}}{FILTER}.subreadset.xml"),
        stats      = format("{{cell}}{FILTER}.cstats.csv"),
    shell:
        # What needs to go into the YML? Stuff from the XML and also some stuff from the
        # stats.
        "touch {output}"

# On GSEG this has to be local as the worker nodes can't see IFS...
if os.environ.get('FILTER_LOCALLY', '0') != '0':
    localrules: filter_control

# This copies the files, optionally applying the nocontrol filter.
rule filter_control:
    output:
        bam    = "{cell}.nocontrol.subreads.bam",
        scraps = "{cell}.nocontrol.scraps.bam",
        xml    = "{cell}.nocontrol.subreadset.xml"
    input:
        bam    = format("{RUNDIR}/{{cell}}.subreads.bam"),
        scraps = format("{RUNDIR}/{{cell}}.scraps.bam"),
        xml    = format("{RUNDIR}/{{cell}}.subreadset.xml")
    run:
        # If the filtering was already done this is a no-op. Check the XML.
        import xml.etree.ElementTree as ET
        root = ET.parse(input['xml']).getroot()
        ns = dict( pbmeta = 'http://pacificbiosciences.com/PacBioCollectionMetadata.xsd' )
        filter_was_done = bool(root.findall('.//pbmeta:ControlKit', ns))

        if filter_was_done:
            # We can just copy it.
            for (infile, outfile) in ( (input[k], output[k]) for k in "bam scraps xml".split() ):
                shell("cp -Lv {infile} {outfile}")

        else:
            # Filter with standard sequences
            shell("{BAM2BAM} {input.bam} {input.scraps} -o {wildcards.cell}.nocontrol")


# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: "( cd `dirname {input}` && md5sum `basename {input}` ) > {output}"

# gzipper that produces byte-identical files each time and discards the input
# But don't bother gzipping BAM files - they are already compressed.
rule gzip_file:
    output: "{file}.gz"
    input: "{file}"
    shell: "gzip -6n {input}"
