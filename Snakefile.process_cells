#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

# This may change in future. Do we need to do the initial filter/copy op
# as a local job?
export FILTER_LOCALLY="$(is_new_cluster && echo 1 || echo 0)"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import yaml
import xml.etree.ElementTree as ET
from snakemake.utils import format
from glob import glob

""" So, this is a recipe to process raw SMRT cells from the PacBio Sequel,
    and start to calculate some stats.

    The script wants to be run in an output dir on /lustre.
    It will process all SMRT cells specified by config['cells']
    If should (!TESTME!) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs):
#   subread_stats.pl
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundir', 'pbpipeline/from')

def scan_cells():
    """ Work out all the cells to process based on config['cells'] and config['rundir']
        and thus infer the base names  of the info.yml files that need to be made.
        Return a dict of cell_id->{'slot': slot_id, 'filter': /(.nocontrol|)/}.
        This in turn allows me to work out everything else by pattern matching.
        When run on a GSEG worker node this list will come out empty, but that's
        OK.
    """
    all_done = [ b[:-len('.transferdone')] for b in glob('{}/*/*.transferdone'.format(RUNDIR) ) ]

    # Now see if I need to filter by config['cells']
    if 'cells' in config:
        res = [ s for s in all_done if s.split('/')[-2] in config['cells'].split() ]
    else:
        res = all_done

    if not ('--no-hooks' in sys.argv or config.get('ignore_missing')):
        # A hacky way of having an assertion only on the master process.
        assert res, "No subreads found in {} matching cell [{}].".format(RUNDIR, config.get('cells', '*'))

    return { r.split('/')[-1]: { 'slot':   r.split('/')[-2],
                                 'filter': determine_filter(r) } for r in res }

def determine_filter(cellpath):
    """ Work out what filter needs to be applied to this cell. The cell is supplied
        as a full path, since scan_cells() already worked that out.
    """
    root = ET.parse(cellpath + '.subreadset.xml').getroot()
    ns = dict( pbmeta = 'http://pacificbiosciences.com/PacBioCollectionMetadata.xsd' )
    filter_was_done = bool(root.findall('.//pbmeta:ControlKit', ns))

    return '' if filter_was_done else '.nocontrol'

# And run the scan just once
SC = scan_cells()
if 'logging' in globals():
    logging.info("SC = {!r}".format(SC))

def find_source_file(extn):
    """ Shim to get me the source file by adding in the cell slot directory as discovered
        by scan_cells()
        I had a different workaround involving linking files but this is more direct.
        This returns a function which can be used as an input function.
    """
    def _ff(wildcards):
        return "{}/{}/{}{}".format(RUNDIR, SC[wildcards.cell]['slot'], wildcards.cell, extn)
    return _ff

def copy_all_files(i, o):
    """ For rules that like to copy stuff
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in o.keys() ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info, link_file
rule main:
    input:
        yaml     = [ c + '.info.yml' for c in SC ],
        seqstats = [ 'sequelstats_plots/' + c + '/.done' for c in SC ]

# This rule connects the .info.yml to all the bits of data we need and also generates
# the .info.yml contents. The inputs get complex since we need to account for whether
# filters need to be applied, and if so we need both the filtered and unfiltered .xml files.
rule one_cell_info:
    output: "{cell}.info.yml"
    input:
        bam_md5    = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".subreads.bam.md5",
        scraps_md5 = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".scraps.bam.md5",
        xml1       = lambda wc: wc.cell + ".subreadset.xml" if SC[wc.cell]['filter'] else [],
        xml2       = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreadset.xml",
        stats      = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreads.cstats.csv",
        scrapstats = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".scraps.cstats.csv",
        #screenpng  = lambda wc: "fqscreen/" + wc.cell + SC[wc.cell]['filter'] + "_screen.png",
        blobs      = lambda wc: "blobplots/" + wc.cell + SC[wc.cell]['filter'] + ".plots.json",
    shell:
        # What needs to go into the YML? Stuff from the XML and also some stuff from the
        # stats, maybe? At present the script will discover extra files automagically.
        "compile_cell_info.py {input.xml2} > {output}"

# On GSEG this has to be local as the worker nodes can't see IFS...
if os.environ.get('FILTER_LOCALLY', '0') != '0':
    localrules: filter_control, copy_reads, copy_xml

# This copies the files, without applying any filter.
rule copy_reads:
    output:
        bam    = "{cell,[^./]+}.subreads.bam",
        pbi    = "{cell,[^./]+}.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.scraps.bam",
    input:
        bam    = find_source_file(".subreads.bam"),
        pbi    = find_source_file(".subreads.bam.pbi"),
        scraps = find_source_file(".scraps.bam"),
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)

# This has to be separate from copy_reads since it happens regardless of the filter applied
rule copy_xml:
    output:
        xml    = "{cell,[^./]+}.subreadset.xml"
    input:
        xml    = find_source_file(".subreadset.xml")
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)

# This applies the control removal filter via bam2bam, resulting in files with .nocontrol added to
# the name and new xml/pbi files.
rule filter_control:
    output:
        bam    = "{cell,[^./]+}.nocontrol.subreads.bam",
        pbi    = "{cell,[^./]+}.nocontrol.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.nocontrol.scraps.bam",
        xml    = "{cell,[^./]+}.nocontrol.subreadset.xml"
    input:
        bam    = find_source_file(".subreads.bam"),
        scraps = find_source_file(".scraps.bam"),
    resources:
        nfscopy=1
    shell:
        # Filter with standard sequences (would we ever use anything else??)
        "{TOOLBOX} bam2bam {input.bam} {input.scraps} -o {wildcards.cell}.nocontrol"

# Note that our contig stats script outputs
#  pc/subread_stats.txt
#  pc/subread_lengths_gc.txt
# I think we just need the former. In any case we definitely need this to be a shadow rule.
rule get_contig_stats_csv:
    output: "{bam}.cstats.csv"
    input:  "{bam}.bam"
    shadow: 'shallow'
    shell:
        '''subread_stats.pl -t 10 -f <({TOOLBOX} samtools fasta {input})
           mv pc/subread_stats.txt {output}
        '''

# A shim rule to write the absolute paths of the .bam files to matching .fn
# files. As long as this runs on the login node sequelstats_stats can run on
# a compute node.
localrules: sequelstats_make_fn
rule sequelstats_make_fn:
    output: temp("{cell}.{part}.bam.fn")
    input:  lambda wc: wc.cell + SC[wc.cell]['filter'] + "." + wc.part + ".bam"
    shell:
        "readlink -f {input} > {output}"

# Computes sequelstats for a single cell, ready to run the plot step above.
# The invocation for sequelstats_pipe is weird but never mind.
# Also we can get failures on bad data.
rule sequelstats_stats:
    output: "sequelstats_plots/{cell}/.done"
    input:
        bamfn    = "{cell}.subreads.bam.fn",
        scrapsfn = "{cell}.scraps.bam.fn",
    shell:
        '''rm -rf sequelstats_plots/{wildcards.cell}/stats
           set +e ; _retval=0
           for step in 01 02 03 04 ; do
             echo STEP_"$step" >&2
             {TOOLBOX} sequelstats_pipe {input.bamfn} {input.scrapsfn} sequelstats_plots 1 STEP_"$step"
             _retval=$(( $_retval + $? ))
           done
           echo $_retval > {output}
        '''

## BLOB plotter rules ##

# I'll copy the logic from Snakefile.common_denovo here

# I'm going to hard-code that there will be 10000 sequences subsampled
# and they will be blasted in 100 chunks.

BAMTOFASTA    = "/lustre/software/bin/samtools fasta"
SEQTK_SAMPLE  = "/lustre/software/bin/seqtk sample"
BLOBTOOLS     = "/lustre/software/blobtools/current_patched/blobtools"
DUSTMASKER    = "/lustre/software/dustmasker/dustmasker_static_2008_04_24"
BLASTN        = "/lustre/software/blast+/ncbi-blast-2.6.0+/bin/blastn"

# Same DB we use for ref-free QC
BLASTDB       = "/lustre/references/blastdb/nt-220217/nt"

# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 plots
localrules: list_blob_plots
rule list_blob_plots:
    output: "blob/{cell}.plots.json"
    input: expand( "blob/{{cell}}.{s}.{taxlevel}.{extn}.png", s = "subreads scraps".split(),
                                                              taxlevel = BLOB_LEVELS,
                                                              extn = "cov0 read_cov.cov0".split() )
    run:
        plots = [ dict(title = 'Taxonomy by {}'.format(taxlevel),
                       files  = expand( "blob/{cell}.{s}.{taxlevel}.{extn}.png",
                                                         cell = [wildcards.cell],
                                                         s = "subreads scraps".split(),
                                                         extn = "cov0 read_cov.cov0".split() )
                      ) for taxlevel in BLOB_LEVELS ]
        with open(output[0], 'w') as ofh:
            print(json.dumps(plots), file=ofh)

# Convert to FASTA and subsample and munge the headers
rule bam_to_subsampled_fasta:
    output: "blob/{foo}+sub{n}.fasta"
    input: "{foo}.bam"
    shell:
        "{BAMTOFASTA} {input} | {SEQTK_SAMPLE} - {wildcards.n} | sed 's,/,_,g' > {output}"


# Makes a .complexity file for our FASTA file
rule fasta_to_complexity:
    output: "blob/{foo}.complexity"
    input: "blob/{foo}.fasta"
    params:
        level = 10
    shell:
        "{DUSTMASKER} -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
rule merge_blast_reports:
    output: "blob/{foo}.blast"
    input: [ "blob/{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        "cat {input} > {output}"

# BLAST a chunk
rule blast_chunk:
    output: "blob/{foo}.blast_part_{chunk}"
    input: "blob/{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        db = BLASTDB,
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{BLASTN} -task dc-megablast -query {input} -db {params.db} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty
rule split_fasta_in_chunks:
    output:
        parts = [ "blob/{{foo}}.fasta_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ],
        list = "blob/{foo}.fasta_parts"
    input: "blob/{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "'`basename {output.list}`'"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
        """mkdir blob_tmp
           {BLOBTOOLS} create -i {input.reads_sample} -o blob_tmp/{wildcards.foo} \
               -t {input.blast_results} -c {input.cov}
           mv blob_tmp/{wildcards.foo}.blobDB.json {output.json}
        """

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
rule blob_plot_png:
    output:
        plotc = "blob/{foo}.{taxlevel}.cov0.png",
        plotr = "blob/{foo}.{taxlevel}.read_cov.cov0.png",
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    shadow: 'shallow'
    shell:
        """ mkdir ./blob_tmp
            export BLOB_COVERAGE_LABEL=Non-Dustiness
            {BLOBTOOLS} blobplot -i {input.json} -o ./blob_tmp/ --sort_first no-hit,other,undef -r {wildcards.taxlevel}
            mv ./blob_tmp/{wildcards.foo}.*.stats.txt {output.stats}
            mv ./blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.cov0.png {output.plotc}
            mv ./blob_tmp/{wildcards.foo}.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png {output.plotr}
        """


## End of BLOB plotter rules ##


# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: "( cd `dirname {input}` && md5sum `basename {input}` ) > {output}"

