#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

# This may change in future. Do we need to do the initial filter/copy op
# as a local job?
export FILTER_LOCALLY="$(is_new_cluster && echo 1 || echo 0)"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import yaml
import xml.etree.ElementTree as ET
from snakemake.utils import format
from glob import glob

""" So, this is a recipe to process raw SMRT cells from the PacBio Sequel,
    and start to calculate some stats.

    The script wants to be run in an output dir on /lustre.
    It will process all SMRT cells specified by config['cells']
    If should (!TESTME!) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])

# Do this so we get a quick error if it's missing.
RUNDIR = config['rundir']

def scan_cells():
    """ Work out all the cells to process based on config['cells'] and config['rundir']
        and thus infer the base names  of the info.yml files that need to be made.
        Return a dict of cell_id->{'slot': slot_id, 'filter': /(.nocontrol|)/}.
        This in turn allows me to work out everything else by pattern matching.
        When run on a GSEG worker node this list will come out empty, but that's
        OK.
    """
    all_subreads = [ b[:-len('.subreads.bam')] for b in glob('{}/*/*.subreads.bam'.format(RUNDIR) ) ]

    # Now see if I need to filter by config['cells']
    if 'cells' in config:
        res = [ s for s in all_subreads if s.split('/')[-2] in config['cells'].split() ]
    else:
        res = all_subreads

    if not ('--no-hooks' in sys.argv or config.get('ignore_missing')):
        # A hacky way of having an assertion only on the master process.
        assert res, "No subreads found in {} matching cell [{}].".format(RUNDIR, config.get('cells', '*'))

    return { r.split('/')[-1]: { 'slot':   r.split('/')[-2],
                                 'filter': determine_filter(r) } for r in res }

def determine_filter(cellpath):
    """ Work out what filter needs to be applied to this cell. The cell is supplied
        as a full path, since scan_cells() already worked that out.
    """
    root = ET.parse(cellpath + '.subreadset.xml').getroot()
    ns = dict( pbmeta = 'http://pacificbiosciences.com/PacBioCollectionMetadata.xsd' )
    filter_was_done = bool(root.findall('.//pbmeta:ControlKit', ns))

    return '' if filter_was_done else '.nocontrol'

# And run the scan just once
SC = scan_cells()

def find_source_file(extn):
    """ Shim to get me the source file by adding in the cell slot directory as discovered
        by scan_cells()
        I had a different workaround involving linking files but this is more direct.
        This returns a function which can be used as an input function.
    """
    def _ff(wildcards):
        return "{}/{}/{}{}".format(RUNDIR, SC[wildcards.cell]['slot'], wildcards.cell, extn)
    return _ff

def copy_all_files(i, o):
    """ For rules that like to copy stuff
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in o ):
        shell("cp -Lv {infile} {outfile}")

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info, link_file
rule main:
    input:
        yaml = [ c + '.info.yml' for c in SC ]

# This rule connects the .info.yml to all the bits of data we need and also generates
# the .info.yml contents. The inputs get complex since we need to account for whether
# filters need to be applied, and if so we need both the filtered and unfiltered .xml files.
rule one_cell_info:
    output: "{cell}.info.yml"
    input:
        bam_md5    = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".subreads.bam.md5",
        scraps_md5 = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".scraps.bam.md5",
        xml1       = lambda wc: wc.cell + ".subreadset.xml" if SC[wc.cell]['filter'] else [],
        xml2       = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreadset.xml",
        stats      = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreads.cstats.csv",
        scrapstats = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".scraps.cstats.csv",
    shell:
        # What needs to go into the YML? Stuff from the XML and also some stuff from the
        # stats, maybe? At present the script will discover extra files automagically.
        "compile_cell_info.py {input.xml2} > {output}"

# On GSEG this has to be local as the worker nodes can't see IFS...
if os.environ.get('FILTER_LOCALLY', '0') != '0':
    localrules: filter_control, copy_reads, copy_xml

# This copies the files, without applying any filter.
rule copy_reads:
    output:
        bam    = "{cell,[^./]+}.subreads.bam",
        pbi    = "{cell,[^./]+}.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.scraps.bam",
    input:
        bam    = find_source_file(".subreads.bam"),
        pbi    = find_source_file(".subreads.bam.pbi"),
        scraps = find_source_file(".scraps.bam"),
    run:
        copy_all_files(input, output)

# This has to be separate since it happens regardless of the filter applied
rule copy_xml:
    output:
        xml    = "{cell,[^./]+}.subreadset.xml"
    input:
        xml    = find_source_file(".subreadset.xml")
    run:
        copy_all_files(input, output)

# This applies the control removal filter via bam2bam, resulting in files with .nocontrol added to
# the name and new xml/pbi files.
rule filter_control:
    output:
        bam    = "{cell,[^./]+}.nocontrol.subreads.bam",
        bam    = "{cell,[^./]+}.nocontrol.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.nocontrol.scraps.bam",
        xml    = "{cell,[^./]+}.nocontrol.subreadset.xml"
    input:
        bam    = find_source_file(".subreads.bam"),
        scraps = find_source_file(".scraps.bam"),
    shell:
        # Filter with standard sequences (would we ever use anything else??)
        shell("{TOOLBOX} bam2bam {input.bam} {input.scraps} -o {wildcards.cell}.nocontrol")

# Note that our contig stats script outputs
#  pc/contig_stats.txt
#  pc/contig_lengths_gc.txt
# I think we just need the former. In any case we definitely need this to be a shadow rule.
rule get_contig_stats_csv:
    output: "{bam}.cstats.csv"
    input:  "{bam}.bam"
    shadow: 'shallow'
    shell:
        '''{TOOLBOX} contig_stats -t 10 -f <({TOOLBOX} samtools fasta {input})
           mv pc/contig_stats.txt {output}
        '''

# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: "( cd `dirname {input}` && md5sum `basename {input}` ) > {output}"

# gzipper that produces byte-identical files each time and discards the input
# But don't bother gzipping BAM files - they are already compressed.
rule gzip_file:
    output: "{file}.gz"
    input: "{file}"
    shell: "gzip -6n {input}"
