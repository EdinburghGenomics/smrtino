#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from snakemake.io import Wildcards
from glob import glob
from smrtino import load_yaml, dump_yaml

""" This is a recipe to process raw SMRT cells from the PacBio Revio,
    and start to calculate some stats.

    The script wants to be run in an output dir under $TO_LOCATION
    It will process all SMRT cells specified by config['cells']
    If should (maybe?) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])
PBCORETOOLS = TOOLBOX + " smrt python3 -m pbcoretools.tasks"

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs).
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# Should not be necessary to give the run dir as pbpipeline/from should point to it...
RUNDIR = config.get('rundir', 'pbpipeline/from')

# Load the info prepared by scan_cells.py
if config.get('noyaml'):
    # Allow for cases where we just want to run one rule and don't care about the sc_data
    SC = dict(cells={}, unfiltered_cells=[None])
else:
    SC = load_yaml(config.get("sc_data", "sc_data.yaml"))

    SC['unfiltered_cells'] = SC['cells']
    if 'cells' in config:
        SC['cells'] =  { k: v for k, v in SC['unfiltered_cells'].items()
                         if v['slot'] in config['cells'].split() }

logger.info(f"SC loaded with cells: {list(SC['unfiltered_cells'])}")
logger.info(f"Of these, we process: {list(SC['cells'])}")

def find_source_file(**args):
    """ Shim to get me a source file by looking into SC.

        This returns a function which can be used as an input function.
    """
    def _fsf(wildcards):
        # combine args and wildcards. This is an undocumented use of the snakemake.io.Wildcards
        # constructor to combine args with wildcards.
        wc = Wildcards(wildcards, args)
        try:
            if wc.barcode == "unassigned":
                # Special case when "unassigned" is used as a barcode name.
                fn = SC['cells'][wc.cell]['unassigned'][wc.part][wc.fmt]
            else:
                fn = SC['cells'][wc.cell]['barcodes'][wc.barcode][wc.part][wc.fmt]

        except AttributeError:
            # No barcode. Must be the metadata file.
            fn = SC['cells'][wc.cell][wc.fmt]

        return f"{RUNDIR}/{fn}"

    return _fsf

def copy_all_files(i, o, keys=None):
    """ For rules that like to copy stuff
        i and o are dicts of filenames with common keys, which
        works for rule input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

def link_all_files(i, o, keys=None):
    """ Copying is not necessary if everything is on the same FS, so have a symlink option.
        (But probably we should have one copy on /lustre-egdp and another on /lustre-gseg.
        Also, copying sorts out the ownership allowing hard-linking for delivery.)

        i and o are dicts of filenames with common keys, which works for rule
        input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        # ln -r resolves out the symlink, so this nasty hack instead...
        ifprefix = "../" * outfile.count("/")
        shell("ln -sn {ifprefix}{infile} {outfile}")

wildcard_constraints:
    n    = r"\d+",
    cell = r"m\w+",
    part = r"hifi_reads|fail_reads",

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info, one_barcode_info, copy_meta
rule main:
    input:
        yaml     = [ f"{c}.info.yaml" for c in SC['cells'] ]

def i_one_cell_info(wc):
    """The summary for a cell is just the summaries for all barcodes, plus unassigned,
       combined with the run info from sc_data.yaml
       The info we need to make this list is in SC.
    """
    cell = wc.cell
    cell_barcodes = sorted(SC['cells'][cell]['barcodes'])

    res = dict( sc_data = "sc_data.yaml" )

    if 'unassigned' in SC['cells'][cell]:
        res['unass'] = f"{cell}/unassigned/{cell}.info.unassigned.yaml"

    res['bc'] = [ f"{cell}/{bc}/{cell}.info.{bc}.yaml"
                  for bc in cell_barcodes ]

    return res

# This rule connects the .info.yaml to all the bits of data we need and also generates
# the .info.yaml contents for a barcode.
def i_one_barcode_info(wc):
    """See what we need to generate for a single barcode. This used to be for a cell but now
       accounts for barcodes by processing each barcode singly.
    """
    cell = wc.cell
    barcode = wc.bc

    # parts = SC['cells'][cell]['parts']

    res = dict( md5 = [],
                cstats = [],
                cou = [],
                xml = [],
                blobs = [],
                taxon = [] )

    # We need md5sums and contig stats for hifi_reads and fail_reads
    for part in "hifi_reads fail_reads".split():
        res['md5'].append(f"md5sums/{cell}/{barcode}/{cell}.{part}.{barcode}.bam.md5")
        res['cstats'].append(f"{cell}/{barcode}/{cell}.{part}.{barcode}.cstats.yaml")

    # Also the consensusreadset.xml, but only for hifi_reads
    res['xml'].append(f"{cell}/{barcode}/{cell}.hifi_reads.{barcode}.consensusreadset.xml")

    # Also the metadata.xml file (which is per cell not per barcode)
    res['metaxml'] = f"{cell}.metadata.xml"

    # For the HIFI reads we want the files in FASTQ format as well as BAM, and we want
    # a .count file and md5sum for that fastq.
    res['cou'].append(f"{cell}/{barcode}/{cell}.hifi_reads.{barcode}.fastq.count")
    res['md5'].append(f"md5sums/{cell}/{barcode}/{cell}.hifi_reads.{barcode}.fastq.gz.md5")

    # Blobs or no? Only on the HiFi reads.
    if str(config.get('blobs', '1')) != '0':
        res['blobs'].append(f"blob/{cell}.hifi_reads.{barcode}.plots.yaml")
        res['taxon'].append(f"blob/{cell}.hifi_reads.{barcode}.species.txt")

    return res

rule one_cell_info:
    output: "{cell}.info.yaml"
    input:  unpack(i_one_cell_info)
    run:
        # Un-silence sys.stderr in sub-jobs:
        logger.quiet.discard('all')

        # The output here is just going to be a YAML file linking to all the other
        # YAML files - there is no point in copying the actual data over.
        try:
            input.unass
            shell("""compile_cell_info.py -x \
                        --sc_data {input.sc_data} \
                        --unassigned {input.unass} \
                        {input.bc} > {output}
                  """)
        except AttributeError:
            # Version with no unassigned
            shell("""compile_cell_info.py -x \
                        --sc_data {input.sc_data} \
                        {input.bc} > {output}
                  """)


rule one_barcode_info:
    output: "{cell}/{bc}/{cell}.info.{bc}.yaml"
    input:  unpack(i_one_barcode_info)
    shell:
       # What needs to go into the YML? Stuff from the XML and also some stuff from the
       # stats, maybe? At present the script will discover extra files automagically.
       # input.xml[0] is the xml for the hifi reads - we don't need to read the XML for
       # the failed reads.
       r"""compile_bc_info.py {input.xml[0]} \
             --metaxml {input.metaxml} \
             --stats  {input.cstats} \
             --taxon  {input.taxon} \
             --plots  {input.blobs} \
             > {output}
        """

# On GSEG this had to be on the login node as the worker nodes can't see FluidFS.
# Still a useful option to have.
# TODO - if we can get the inputs on a different FS again then revert to copying
if os.environ.get('FILTER_LOCALLY', '1') != '0':
    localrules: copy_reads

def relativize_xml(xfile):
    """Munge an XML file to reflect new file paths and remove unused references
       so that the file can be imported into another SMRTLink.
    """
    shell("mv -v {xfile} {xfile}.orig")
    shell("strip_readset_resources.py {xfile}.orig > {xfile}")
    shell("{TOOLBOX} smrt dataset --skipCounts --log-level INFO relativize {xfile}")
    shell("rm -v {xfile}.orig")

# This copies/links the files for a given barcode, then fixes up the XML.
# part may be "hifi_reads" (ie. pass) or "fail_reads".
rule copy_reads:
    output:
        bam    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam",
        pbi    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam.pbi",
        xml    = "{cell}/{barcode}/{cell}.{part}.{barcode}.consensusreadset.xml",
    input:
        bam    = find_source_file(fmt="bam"),
        pbi    = find_source_file(fmt="pbi"),
        xml    = find_source_file(fmt="xml"),
    resources:
        nfscopy=1
    run:
        # Un-silence sys.stderr in sub-jobs:
        logger.quiet.discard('all')

        copy_all_files(input, output, keys="bam pbi".split())
        copy_all_files(input, output, keys="xml".split())
        relativize_xml(output.xml)

# This file just gets copied. Except that we have to fix the XML declaration.
rule copy_meta:
    output:
        meta   = "{cell}.metadata.xml"
    input:
        meta   = find_source_file(fmt="meta")
    shell:
        "sed '1s/utf-16/utf-8/' {input.meta} > {output.meta}"

# This script produces some headline stats as well as a histogram we could use
# (but currently we don't)
rule get_cstats_yaml:
    output:
        yaml = "{bam}.cstats.yaml",
        histo = "histo/{bam}.length_histo.tsv"
    input:  "{bam}.bam"
    threads: 6
    resources:
        mem_mb = 36000,
        n_cpus = 6,
    shell:
        "fasta_stats.py -H {output.histo} <({TOOLBOX} samtools fasta -@ {threads} {input}) > {output.yaml}"

# Export the HiFi reads (or any BAM file) as FASTQ. Assumes all FASTQ files are derived from
# BAM files.
# My logic on using $(( {threads} / 2 }} for both compression and decompression is that
# decompression is faster but the FASTQ (which has no kinetics) is much smaller. But I've
# not really tested if this is optimal.
rule bam_to_fastq:
    output: "{foo}.fastq.gz"
    input:  "{foo}.bam"
    threads: 16
    resources:
        mem_mb = 108000,
        n_cpus = 16,
    shell:
       r"""sam_threads=$((  ({threads} > 4) ? ({threads} / 4)     : 1 ))
           pigz_threads=$(( ({threads} > 2) ? (3 * {threads} / 4) : 1 ))
           {TOOLBOX} samtools fastq -@ $sam_threads {input} | pigz -c -n -p $pigz_threads > {output}
        """

# Make a .count file for the FASTQ file
rule count_fastq:
    output: "{foo}.fastq.count"
    input:  "{foo}.fastq.gz"
    shell:  "fq_base_counter.py {input} > {output}"

# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: '( cd "$(dirname {input:q})" && md5sum -- "$(basename {input:q})" ) > {output:q}'

## BLOB plotter rules ##
include: "Snakefile.blob"
