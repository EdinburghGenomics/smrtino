#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from snakemake.io import Wildcards
from glob import glob
from smrtino import load_yaml, dump_yaml


""" This is a recipe to process raw SMRT cells from the PacBio Revio,
    and start to calculate some stats.

    The script wants to be run in an output dir under $TO_LOCATION
    It will process all SMRT cells specified by config['cells']
    If should (maybe?) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])
PBCORETOOLS = TOOLBOX + " smrt python3 -m pbcoretools.tasks"

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs).
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# Should not be necessary to give the run dir as pbpipeline/from should point to it...
RUNDIR = config.get('rundir', 'pbpipeline/from')

# Load the info prepared by scan_cells.py
try:
    SC = load_yaml(config.get("sc_data", "sc_data.yaml"))
except Exception:
    # Allow for cases where we just want to run one rule and don't care about the sc_data
    SC = dict(cells={})

logger.info(f"SC =\n{SC!r}")

def find_source_file(**args):
    """ Shim to get me a source file by looking into SC.

        This returns a function which can be used as an input function.
    """
    def _fsf(wildcards):
        # combine args and wildcards. This is an undocumented use of the snakemake.io.Wildcards
        # constructor to combine args with wildcards.
        wc = Wildcards(wildcards, args)
        try:
            if wc.barcode == "unassigned":
                # Special case when "unassigned" is used as a barcode name.
                fn = SC['cells'][wc.cell]['unassigned'][wc.part][wc.fmt]
            else:
                fn = SC['cells'][wc.cell]['barcodes'][wc.barcode][wc.part][wc.fmt]

        except AttributeError:
            # No barcode. Must be the metadata file.
            fn = SC['cells'][wc.cell][wc.fmt]

        return f"{RUNDIR}/{fn}"

    return _fsf

def copy_all_files(i, o, keys=None):
    """ For rules that like to copy stuff
        i and o are dicts of filenames with common keys, which
        works for rule input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

def link_all_files(i, o, keys=None):
    """ Copying is not necessary if everything is on the same FS,
        so have a symlink option. But probably we should have one
        copy on /lustre-egdp and another on /lustre-gseg.
        i and o are dicts of filenames with common keys, which
        works for rule input/output with matched names
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in (keys or o.keys()) ):
        shell("ln -sn {infile} {outfile}")

wildcard_constraints:
    cell = r"m\w+",
    part = r"hifi_reads|fail_reads",

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info
rule main:
    input:
        yaml     = [ f"{c}.info.yaml" for c in SC['cells'] ]

def i_one_cell_info(wc):
    """The summary for a cell is just the summaries for all barcodes, plus unassigned,
       combined with the run info from sc_data.yaml
       The info we need to make this list is in SC.
    """
    cell = wc.cell
    cell_barcodes = sorted(SC['cells'][cell]['barcodes'])

    res = dict( sc_data = "sc_data.yaml",
                unass   = f"{cell}/unassigned/{cell}.info.unassigned.yaml")

    res['bc'] = [ f"{cell}/{bc}/{cell}.info.{bc}.yaml"
                  for bc in cell_barcodes ]

    return res

# This rule connects the .info.yaml to all the bits of data we need and also generates
# the .info.yaml contents for a barcode.
def i_one_barcode_info(wc):
    """See what we need to generate for a single barcode. This used to be for a cell but now
       accounts for barcodes by processing each barcode singly.
    """
    '''
    scinfo = SC['cells'][cell]
    barcodes = scinfo['barcodes']
    barcodes_and_unass = dict( unassigned = scinfo['unassigned'],
                               **barcodes )
    '''
    cell = wc.cell
    barcode = wc.bc

    # parts = SC['cells'][cell]['parts']

    res = dict( md5 = [],
                cstats = [],
                cou = [],
                xml = [],
                blobs = [],
                taxon = [] )

    # We need md5sums and contig stats for hifi_reads and fail_reads
    for part in "hifi_reads fail_reads".split():
        res['md5'].append(f"md5sums/{cell}/{barcode}/{cell}.{part}.{barcode}.bam.md5")
        res['cstats'].append(f"{cell}/{barcode}/{cell}.{part}.{barcode}.cstats.yaml")

        # Also the consensusreadset.xml
        res['xml'].append(f"{cell}/{barcode}/{cell}.{part}.{barcode}.consensusreadset.xml")

    # Also the metadata.xml file (which is per cell not per barcode)
    res['metaxml'] = f"{cell}.metadata.xml"

    # For the HIFI reads we want the files in FASTQ format as well as BAM, and we want
    # a .count file and md5sum for that fastq.
    res['cou'].append(f"{cell}/{barcode}/{cell}.hifi_reads.{barcode}.fastq.count")
    res['md5'].append(f"md5sums/{cell}/{barcode}/{cell}.hifi_reads.{barcode}.fastq.gz.md5")

    # Blobs or no? Only on the HiFi reads.
    if str(config.get('blobs', '1')) != '0':
        res['blobs'].append(f"blob/{cell}.hifi_reads.{barcode}.plots.yaml")
        res['taxon'].append(f"blob/{cell}.hifi_reads.{barcode}.species.txt")

    return res

rule one_cell_info:
    output: "{cell}.info.yaml"
    input:  unpack(i_one_cell_info)
    shell:
       r"""combine_cell_info.py \
                --sc_data {input.sc_data} \
                --unass {input.unass} \
                {input.bc} > {output}
        """

rule one_barcode_info:
    output: "{cell}/{bc}/{cell}.info.{bc}.yaml"
    input:  unpack(i_one_barcode_info)
    shell:
       # What needs to go into the YML? Stuff from the XML and also some stuff from the
       # stats, maybe? At present the script will discover extra files automagically.
       r"""compile_bc_info.py {input.xml[0]} \
             --runxml {input.metaxml} \
             --stats  {input.cstats} \
             --taxon  {input.taxon} \
             --plots  {input.blobs} > {output}
        """

# On GSEG this had to be on the login node as the worker nodes can't see FluidFS.
# Still a useful option to have.
# TODO - if we can get the inputs on a different FS again then revert to copying
if os.environ.get('FILTER_LOCALLY', '1') != '0':
    localrules: copy_reads

def relativize_xml(xfile):
    """Munge an XML file to reflect new file paths and remove unused references
       so that the file can be imported into another SMRTLink.
    """
    shell("mv -v {xfile} {xfile}.orig")
    shell("strip_readset_resources.py {xfile}.orig > {xfile}")
    shell("{TOOLBOX} smrt dataset --skipCounts --log-level INFO relativize {xfile}")
    shell("rm -v {xfile}.orig")

# This copies/links the files for a given barcode, then fixes up the XML.
# part may be "hifi_reads" (ie. pass) or "fail_reads".
rule copy_reads:
    output:
        bam    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam",
        pbi    = "{cell}/{barcode}/{cell}.{part}.{barcode}.bam.pbi",
        xml    = "{cell}/{barcode}/{cell}.{part}.{barcode}.consensusreadset.xml",
    input:
        bam    = find_source_file(fmt="bam"),
        pbi    = find_source_file(fmt="pbi"),
        xml    = find_source_file(fmt="xml"),
    resources:
        nfscopy=1
    run:
        link_all_files(input, output, keys="bam pbi".split())
        copy_all_files(input, output, keys="xml".split())
        relativize_xml(output.xml)

# This file just gets copied. Except that we have to fix the XML declaration.
rule copy_meta:
    output:
        meta   = "{cell}.metadata.xml"
    input:
        meta   = find_source_file(fmt="meta")
    shell:
        "sed '1s/utf-16/utf-8/' {input.meta} > {output.meta}"

# This script produces some headline stats as well as a histogram we can use
# (but currently we don't)
rule get_contig_stats_yaml:
    output:
        yaml = "{bam}.cstats.yaml",
        histo = "histo/{bam}.length_histo.tsv"
    input:  "{bam}.bam"
    shell:
        "fasta_stats.py -H {output.histo} <({TOOLBOX} samtools fasta {input}) > {output.yaml}"

# Export the HiFi reads (or any BAM file) as FASTQ. Assumes all FASTQ files are derived from
# BAM files.
rule bam_to_fastq:
    output: "{foo}.fastq.gz"
    input:  "{foo}.bam"
    threads: 4
    resources:
        mem_mb = 24000,
        n_cpus = 4,
    shell:
        "{TOOLBOX} samtools fastq {input} | pigz -c -n -p {threads} > {output}"

# Make a .count file for the FASTQ file
rule count_fastq:
    output: "{foo}.fastq.count"
    input:  "{foo}.fastq.gz"
    shell:  "fq_base_counter.py {input} > {output}"

# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: '( cd "$(dirname {input:q})" && md5sum -- "$(basename {input:q})" ) > {output:q}'

## BLOB plotter rules ##
include: "Snakefile.blob"
