#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# Note this workflow normally expects to run on gseg-login0 while viewing files
# in RUNDIR on /fluidfs, which cannot be seen by the worker nodes.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

# This may change in future. Do we need to do the initial filter/copy op
# as a local job?
export FILTER_LOCALLY="$(is_new_cluster && echo 1 || echo 0)"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
import yaml
import xml.etree.ElementTree as ET
from snakemake.utils import format
from glob import glob

""" So, this is a recipe to process raw SMRT cells from the PacBio Sequel,
    and start to calculate some stats.

    The script wants to be run in an output dir on /lustre.
    It will process all SMRT cells specified by config['cells']
    If should (!TESTME!) be possible to run multiple instances at once, so long as the
    cells are not conflicting.
"""
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])

# Other than that, ensure that scripts in the directory with this Snakefile are
# in the PATH (we have to do it here as $PATH does not get preserved on cluster jobs):
#   subread_stats.pl
if ( not os.path.dirname(workflow.snakefile) in os.environ['PATH'].split(':') and
     not os.path.dirname(os.path.abspath(workflow.snakefile)) in os.environ['PATH'].split(':') ):
     os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundir', 'pbpipeline/from')

def scan_cells():
    """ Work out all the cells to process based on config['cells'] and config['rundir']
        and thus infer the base names  of the info.yml files that need to be made.
        Return a dict of cell_id->{'slot': slot_id, 'filter': /(.nocontrol|)/}.
        This in turn allows me to work out everything else by pattern matching.
        When run on a GSEG worker node this list will come out empty, but that's
        OK.
    """
    all_done = [ b[:-len('.transferdone')] for b in glob('{}/*/*.transferdone'.format(RUNDIR) ) ]

    # Now see if I need to filter by config['cells']
    if 'cells' in config:
        res = [ s for s in all_done if s.split('/')[-2] in config['cells'].split() ]
    else:
        res = all_done

    if not ('--no-hooks' in sys.argv or config.get('ignore_missing')):
        # A hacky way of having an assertion only on the master process.
        assert res, "No subreads found in {} matching cell [{}].".format(RUNDIR, config.get('cells', '*'))

    return { r.split('/')[-1]: { 'slot':   r.split('/')[-2],
                                 'filter': determine_filter(r) } for r in res }

def determine_filter(cellpath):
    """ Work out what filter needs to be applied to this cell. The cell is supplied
        as a full path, since scan_cells() already worked that out.
    """
    root = ET.parse(cellpath + '.subreadset.xml').getroot()
    ns = dict( pbmeta = 'http://pacificbiosciences.com/PacBioCollectionMetadata.xsd' )
    filter_was_done = bool(root.findall('.//pbmeta:ControlKit', ns))

    return '' if filter_was_done else '.nocontrol'

# And run the scan just once
SC = scan_cells()
if 'logging' in globals():
    logging.info("SC = {!r}".format(SC))

def find_source_file(extn):
    """ Shim to get me the source file by adding in the cell slot directory as discovered
        by scan_cells()
        I had a different workaround involving linking files but this is more direct.
        This returns a function which can be used as an input function.
    """
    def _ff(wildcards):
        return "{}/{}/{}{}".format(RUNDIR, SC[wildcards.cell]['slot'], wildcards.cell, extn)
    return _ff

def copy_all_files(i, o):
    """ For rules that like to copy stuff
    """
    for (infile, outfile) in ( (i[k], o[k]) for k in o.keys() ):
        shell("cp --no-preserve=all -Lv {infile} {outfile}")

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info, link_file
rule main:
    input:
        yaml     = [ c + '.info.yml' for c in SC ],
        seqstats = [ 'sequelstats_plots/' + c + '/.done' for c in SC ]

# This rule connects the .info.yml to all the bits of data we need and also generates
# the .info.yml contents. The inputs get complex since we need to account for whether
# filters need to be applied, and if so we need both the filtered and unfiltered .xml files.
rule one_cell_info:
    output: "{cell}.info.yml"
    input:
        bam_md5    = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".subreads.bam.md5",
        scraps_md5 = lambda wc: "md5sums/" + wc.cell + SC[wc.cell]['filter'] + ".scraps.bam.md5",
        xml1       = lambda wc: wc.cell + ".subreadset.xml" if SC[wc.cell]['filter'] else [],
        xml2       = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreadset.xml",
        stats      = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".subreads.cstats.csv",
        scrapstats = lambda wc: wc.cell + SC[wc.cell]['filter'] + ".scraps.cstats.csv",
        screenpng  = lambda wc: "fqscreen/" + wc.cell + SC[wc.cell]['filter'] + "_screen.png",
    shell:
        # What needs to go into the YML? Stuff from the XML and also some stuff from the
        # stats, maybe? At present the script will discover extra files automagically.
        "compile_cell_info.py {input.xml2} > {output}"

# On GSEG this has to be local as the worker nodes can't see IFS...
if os.environ.get('FILTER_LOCALLY', '0') != '0':
    localrules: filter_control, copy_reads, copy_xml

# This copies the files, without applying any filter.
rule copy_reads:
    output:
        bam    = "{cell,[^./]+}.subreads.bam",
        pbi    = "{cell,[^./]+}.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.scraps.bam",
    input:
        bam    = find_source_file(".subreads.bam"),
        pbi    = find_source_file(".subreads.bam.pbi"),
        scraps = find_source_file(".scraps.bam"),
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)

# This has to be separate from copy_reads since it happens regardless of the filter applied
rule copy_xml:
    output:
        xml    = "{cell,[^./]+}.subreadset.xml"
    input:
        xml    = find_source_file(".subreadset.xml")
    resources:
        nfscopy=1
    run:
        copy_all_files(input, output)

# This applies the control removal filter via bam2bam, resulting in files with .nocontrol added to
# the name and new xml/pbi files.
rule filter_control:
    output:
        bam    = "{cell,[^./]+}.nocontrol.subreads.bam",
        pbi    = "{cell,[^./]+}.nocontrol.subreads.bam.pbi",
        scraps = "{cell,[^./]+}.nocontrol.scraps.bam",
        xml    = "{cell,[^./]+}.nocontrol.subreadset.xml"
    input:
        bam    = find_source_file(".subreads.bam"),
        scraps = find_source_file(".scraps.bam"),
    resources:
        nfscopy=1
    shell:
        # Filter with standard sequences (would we ever use anything else??)
        "{TOOLBOX} bam2bam {input.bam} {input.scraps} -o {wildcards.cell}.nocontrol"

# Note that our contig stats script outputs
#  pc/subread_stats.txt
#  pc/subread_lengths_gc.txt
# I think we just need the former. In any case we definitely need this to be a shadow rule.
rule get_contig_stats_csv:
    output: "{bam}.cstats.csv"
    input:  "{bam}.bam"
    shadow: 'shallow'
    shell:
        '''subread_stats.pl -t 10 -f <({TOOLBOX} samtools fasta {input})
           mv pc/subread_stats.txt {output}
        '''

# A shim rule to write the absolute paths of the .bam files to matching .fn
# files. As long as this runs on the login node sequelstats_stats can run on
# a compute node.
localrules: sequelstats_make_fn
rule sequelstats_make_fn:
    output: temp("{cell}.{part}.bam.fn")
    input:  lambda wc: wc.cell + SC[wc.cell]['filter'] + "." + wc.part + ".bam"
    shell:
        "readlink -f {input} > {output}"

# Computes sequelstats for a single cell, ready to run the plot step above.
# The invocation for sequelstats_pipe is weird but never mind.
# Also we can get failures on bad data.
rule sequelstats_stats:
    output: "sequelstats_plots/{cell}/.done"
    input:
        bamfn    = "{cell}.subreads.bam.fn",
        scrapsfn = "{cell}.scraps.bam.fn",
    shell:
        '''rm -rf sequelstats_plots/{wildcards.cell}/stats
           set +e ; _retval=0
           for step in 01 02 03 04 ; do
             echo STEP_"$step" >&2
             {TOOLBOX} sequelstats_pipe {input.bamfn} {input.scrapsfn} sequelstats_plots 1 STEP_"$step"
             _retval=$(( $_retval + $? ))
           done
           echo $_retval > {output}
        '''

# Generates a fastqscreen plot. Somewhat different to Illuminatus as the reads are
# different lengths. I'm going to generate and trim and filter the CCS
# outside of this rule. But I'll still let fastq_screen do the subsetting.
# Renaming the outputs of fastq_screen is a tiny bit tricky.
rule fqscreen:
    output:
        txt  = "fqscreen/{cell}_screen.txt",
        html = "fqscreen/{cell}_screen.html",
        png  = "fqscreen/{cell}_screen.png"
    input: "{cell}.ccs+trim50.fastq.gz"
    shadow: 'shallow'
    threads: 2
    params:
        subset = 1000000
    shell:
        """if [ -s {input} ] ; then
             touch {output.txt} {output.html}
             {TOOLBOX} gm_label.sh 1400x200 'Unable to run fastq_screen for {input}' {output.png}
           else
             {TOOLBOX} fastq_screen --threads {threads} --bowtie '' --subset {params.subset} {input}
             for f in {output} ; do
                  extn="${{f##*{wildcards.cell}}}"
                  mv {wildcards.cell}.ccs+trim*"$extn" "$f"
             done
           fi
        """

# Generates the CCS for a raw BAM file. Using minpass 2 seems to make sense to get as many
# reads as possible for fastq_screen.
rule smrt_ccs:
    output:
        bam = "{cell}.ccs.bam",
        pbi = "{cell}.ccs.bam.pbi",
        rep = "{cell}_ccs_report.txt",
    input:  "{cell}.subreads.bam"
    shadow: 'shallow'
    params:
        minlen = 50,
        minpass = 2
    shell:
        """{TOOLBOX} smrt ccs --minLength={params.minlen} --minPasses={params.minpass} \
           --reportFile={output.rep} {input} {output.bam}
        """


# Trim reads in a FASTQ file to length N (borrowed from qc_tools_python) and also discards
# any reads shorter than N.
# If there is an error, make an empty file so the pipeline can continue.
# For the purposes of this pipeline I'm working directly from the BAM, but we may want to make
# the full CCS FASTQ at some point too?
# Also is there any reason to take the read from a point other than the first base? Any quality
# issues at the beginning of the read?
rule trim_to_n:
    output:
        fastq = "{cell}.ccs+trim{trim,\d+}.fastq.gz"
    input:
        bam   = "{cell}.ccs.bam"
    threads: 2
    shell:
        """{TOOLBOX} samtools fastq {input.bam} | {TOOLBOX} fastx_trimmer -l {wildcards.trim} -m {wildcards.trim} | \
           pigz -p {threads} -6c >{output.fastq} \
            || touch {output.fastq}
        """

# md5summer that keeps the file path out of the .md5 file
rule md5sum_file:
    output: "md5sums/{foo}.md5"
    input: "{foo}"
    shell: "( cd `dirname {input}` && md5sum `basename {input}` ) > {output}"

# gzipper that produces byte-identical files each time and discards the input
# But don't bother gzipping BAM files - they are already compressed.
rule gzip_file:
    output: "{file}.gz"
    input: "{file}"
    shell: "gzip -6n {input}"
